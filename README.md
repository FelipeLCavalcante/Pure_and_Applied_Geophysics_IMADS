# Interpreting magnetic anomalies in dike swarms: Deterministic and probabilistic approaches  

**Autores:** Felipe L. Cavalcante<sup>1</sup>, Carlos A. Mendonça<sup>2</sup>, Ulrich S. Ofterdinger<sup>3</sup>, Mark Cooper<sup>4</sup>       

<sup>1</sup> Center for Energy and Petroleum Studies, University of Campinas, Cora Coralina Street, 350, Campinas, SP, Brazil  
<sup>2</sup> Department of Geophysics, University of São Paulo, Rua do Matão, 1226, São Paulo, SP, Brazil  
<sup>3</sup> School of Natural and Built Environment, Queen’s Univesity Belfast, David Keir Building, Stranmillis Road, Belfast, United Kingdom   
<sup>4</sup> British Geological Survey, Dundonald House, Upper Newtownards Road, Belfast, United Kingdom  

Correspondence: felipe.cavalcante@alumni.usp.br  

### Abstract
Magnetic anomaly-derived lineaments, indicative of major dike swarms at shallow crustal levels, are common features associated with igneous provinces because of the contrasting magnetization of diabase and dolerites with respect to crystalline and sedimentary country rocks. Aeromagnetic datasets are useful to track the extension of subsurface structures, but rarely do the observed magnetic anomalies directly match the exposed dikes. Usually, a large number of dikes are inferred from airborne data, many of them seated at deeper levels with few reaching the ground surface. To fully understand the effects of dikes in groundwater, geothermal, and petroleum and gas geosystems, accurate mapping of dike distribution and intrusion depths is required. Dikes are usually steeply dipping, elongate, and tabular bodies, and this allows them to be represented as simple vertical thin-sheet models not explicitly accounting for their width. Exploring the correspondence of this parametrization, regarding the equivalence of the amplitude of the magnetic anomaly (AMA) with a field generated by a line of current situated at the thin- sheet top, it is possible to formulate the resulting field as equivalent to a Cauchy probability density distribution. This equivalence allows model uncertainty inferences about depth estimates solely based on the observed AMA attributes, gives reliable automatic solutions for preliminary geological interpretation, and provides well-suited initial solutions to fast convergence of nonlinear data fitting to the observed magnetic anomaly. The developed procedure is applied to interpret a magnetic transect across dike swarms of the North Atlantic Igneous Province in Northern Ireland for which a minimum number of dikes at variable depths are inferred to explain the observed magnetic transect.

## About
Repository containing scripts for 2D magnetic forward modeling and inversion of thin dikes.   

## Index

- [Repository Contents](#repository-contents)
- [Folder Structure](#folder-structure)
- [Technical Specifications](#technical-specifications)
- [Instructions for Use](#instructions-for-use)
- [License](#license)

## Repository Contents   

This repository contains the files necessary to reproduce the results obtained in this research. The content is organized into folders that contain:

- **Scripts:** used for forward modeling, processing, inversion, and data analysis.

## Folder Structure

The organization of this repository follows the structure below:

```plaintext
Minerals_IMADS/
├── README.md                    # This file
├── PyDyke_env.yml               # Virtual environment configuration file
├── FWD/ (folder of the forward problem)
│   ├── Input/
│   │   └── (input files for running the forward problem)
│   ├── Output/
│   │   └── (output files from the forward problem)
│   ├── 1_PyDyke_Extent_n_Disturb.py (file for executing the forward problem)
│   ├── 2_PyDyke_cut_data.py (file for cropping the data)
│   ├── func_*.py (files containing functions for executing the forward problem)
├── INV/ (folder of the inverse problem)
│   ├── Input/
│   │   └── (input files for running the inverse problem)
│   ├── Output/
│   │   └── (output files from the inverse problem)
│   ├── 1_PyDyke_simple_filt.py (file for filtering data)
│   ├── 2_PyDyke_d2T.py (file for calculating the second derivative of the data)
│   ├── 3_PyDyke_lims.py (file for setting model limits)
│   ├── 4_PyDyke_calc_initial.py (file for calculating the initial solution for the problem)
│   ├── 5_PyDyke_inv.py (file for executing the inverse problem)
│   ├── 6_PyDyke_plot.py (file for plotting the results of the inverse problem)
│   ├── 7_PyDyke_STD.py (file for calculating and plotting the standard deviation of the solutions)
│   ├── 8_PyDyke_join_exit_std.py (file for compiling solutions and probabilities into a table)
│   ├── func_*.py (files containing functions for executing the inverse problem)
```


## Technical Specifications

Original codes and tests were developed on a notebook with the following specifications:

- **Processor:** 12th Generation Intel Core i5 (3.3 GHz)
- **Memory:** 16 GB DDR4 RAM (3200 MHz)
- **Operating System:** Ubuntu 24.04

The following software and programming languages were used:

### Python Environment
- Python 3.7.16

## Instructions for Use

The proper functioning of the programs depends on installing the required libraries and packages.  
This can be easily resolved in Linux-based systems through 4 steps: 
  1. Download the file `PyDyke_env.yml`.
  2. Create a virtual environment with Conda, replacing "novo_env" with the desired environment name, using the command: ```conda env create --name novo_env -f PyDyke.yml```
   3. Activate the virtual environment, replacing "nome_do_ambiente" with the name of the created environment, with the command: ```conda activate nome_do_ambiente```

Once the virtual environment is configured, the Python codes can be executed with commands like ```python 1_exemplo.py```, or by using your favorite IDE's buttons. The execution order of the codes follows the ascending Arabic numeral order of the ".py" file prefixes. Download the files into two separate folders, one for the forward problem and another for the inverse problem. Create the "Input" and "Output" folders within each folder before running the scripts for the first time.
